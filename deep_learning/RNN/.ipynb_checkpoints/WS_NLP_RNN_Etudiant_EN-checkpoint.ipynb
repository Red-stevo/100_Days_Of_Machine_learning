{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1U5AyPhBw2z",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Workshop-Skills\" data-toc-modified-id=\"Workshop-Skills-1\"><span class=\"toc-item-num\">1&nbsp; &nbsp;</span>-Workshop-Skills</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-2\"><span class=\"toc-item-num\">2&nbsp; &nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Philosophy-and-recommendations\" data-toc-modified-id=\"Philosophy-and-recommendations-3\"><span class=\"toc-item-num\">3&nbsp; &nbsp;</span>Philosophy and recommendations</a></span></li><li><span><a href=\"#Pretreatment-and-environmental-preparation\" data-toc-modified-id=\"Pretreatment-and-environmental-preparation-4\"><span class=\"toc-item-num\">4&nbsp; &nbsp;</span>Environmental pretreatment and preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-tweets\" data-toc-modified-id=\"Cleaning-tweets-4. 1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Tweets cleanup</a></span></li><span><a href=\"#Tokenization-or-segmentation\" data-toc-modified-id=\"Tokenization-or-segmentation-4.2\"><span class=\"toc-item-num\">4. 2&nbsp;&nbsp;</span>Tokenization or segmentation</a></span></li><li><span><a href=\"#Word-Embedding:-convertir-un-document-en-une-matrice-de-nombres\" data-toc-modified-id=\"Word-Embedding:-convertir-un-document-en-une-matrice-de-nombres-4. 3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Word Embedding: convert a document into a matrix of numbers</a></span></li><li><span><a href=\"#Pipeline-Preparation\" data-toc-modified-id=\"Pipeline-Preparation-4.4\"><span class=\"toc-item-num\">4. 4&nbsp;&nbsp;</span>Pipeline preparation</a></span></li></ul></li><span><a href=\"#Model-training\" data-toc-modified-id=\"Model-training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model-training</a></span><ul class=\"toc-item\"><li><span><a href=\"#RNN: -Simple-recurrent-neuron-networks\" data-toc-modified-id=\"RNN:-Simple-recurrent-neuron-networks-5. 1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>RNN: Simple recurrent neural networks</a></span></li><span><a href=\"#GRU:-Gated-Recurrent-Units\" data-toc-modified-id=\"GRU:-Gated-Recurrent-Units-5. 2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>GRU: Gated Recurrent Units</a></span></li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-5.3\"><span class=\"toc-item-num\">5. 3&nbsp;&nbsp;</span>LSTM</a></span></li></ul></li><li><span><a href=\"#Evaluation-of-model-performance\" data-toc-modified-id=\"Evaluation-of-model-performance-6\"><span class=\"toc-item-num\">6&nbsp; &nbsp;</span>Performance evaluation of models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluation-of-models-by-the-dataset-test\" data-toc-modified-id=\"Evaluation-of-models-by-the-dataset-test-6. 1\"><span class=\"toc-item-num\">6. 1&nbsp;&nbsp;</span>Evaluation of models by the test dataset</a></span></li><li><span><a href=\"#Evaluation-of-LSTM-results\" data-toc-modified-id=\"Evaluation-of-LSTM-results-6.2\"><span class=\"toc-item-num\">6. 2&nbsp;&nbsp;</span>Evaluation of LSTM results</a></span></li><li><span><a href=\"#Pour-aller-plus-loin-avec-NLP\" data-toc-modified-id=\"Pour-aller-plus-loin-avec-NLP-6. 3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>To go further with NLP</a></span></li></ul></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_QzVwZVG9L5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UT_kRAHG6AH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEbQCkM_Bw22"
   },
   "source": [
    "\n",
    "# Natural language processing and classification using Recurrent Neural Networks (RNNs)\n",
    "\n",
    "![GPI-CESI.jpg](attachment:GPI-CESI.jpg)\n",
    "                    \n",
    "<table>\n",
    "<thead>\n",
    "  <tr><th>Author</th><th>Reader</th><th>Center</th><th>Editor</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr><td>Genane YOUNESS</td><td>Benjamin COHEN BOULAKIA</td><td>Nanterre</td><td>2023-02-07</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtLPMkqmBw23"
   },
   "source": [
    "The aim of this Workshop is to introduce you to the basics of NLP (Natural Language Processing)(https://lbourdois.github.io/blog/nlp/) and to build a binary classification model using recurrent neural networks (RNNs).\n",
    "\n",
    "Unlike traditional neural networks, recurrent neural networks span spatial and temporal sequences. In other words, the hidden layers of the present moment and the next moment are linked.\n",
    "The application you are about to carry out involves building a model to identify Twitter posts (\"tweets\") announcing a disaster. This learning is part of natural language processing, NLP.\n",
    "This classification of binary text is important, as it could help state agencies to quickly identify and respond to disasters.\n",
    "The data available are tagged tweets reporting a disaster or not.\n",
    "First, we'll clean up the text data before moving on to binary classification with RNN.\n",
    "\n",
    "The dataset used in this Workshop is that of the Kaggle challenge [_Natural Language Processing with Disaster Tweets_](https://www.kaggle.com/competitions/nlp-getting-started/data). This dataset is provided to you in an archive <code>nlp-getting-started.zip</code>, unzip its contents into a directory <code>nlp-getting-started</code> which you will place in the same directory as this Workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrQNrRnaBw24"
   },
   "source": [
    "## Pre-treatment and environmental preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vQ2ggE4-Bw25"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV3YVCm7Bw26"
   },
   "source": [
    "We start by downloading the Twitter publications (\"tweets\"). We retrieve the columns we're interested in, which are the <code>texts</code> and <code>target</code> columns. The text is a sentence or tweet of type _text_, and the _target_ is of type _int64_ labeled <code>1</code> for catastrophe, <code>0</code> otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5Hro5S1YBw26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change according to your dataset download path\n",
    "# os.chdir('')\n",
    "train_data = pd.read_csv('train.csv', usecols=['text', 'target'], dtype={'text': str, 'target': np.int64})\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[:5, :]\n",
    "\n",
    "# train_data = train_data.dropna()\n",
    "\n",
    "train_data.iloc[:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufrdwEojBw26"
   },
   "source": [
    "We will also download test data to evaluate the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2 = pd.read_csv('./train.csv')\n",
    "train2.iloc[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./test.csv')\n",
    "test_data.iloc[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "TbhlLqRtBw26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    Our Deeds are the Reason of this #earthquake M...\n",
       " 1               Forest fire near La Ronge Sask. Canada\n",
       " 2    All residents asked to 'shelter in place' are ...\n",
       " 3    13,000 people receive #wildfires evacuation or...\n",
       " 4    Just got sent this photo from Ruby #Alaska as ...\n",
       " Name: text, dtype: object,\n",
       " 0    1\n",
       " 1    1\n",
       " 2    1\n",
       " 3    1\n",
       " 4    1\n",
       " Name: target, dtype: int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_data['text']\n",
    "y = train_data['target']\n",
    "\n",
    "x[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    4308\n",
      "1    3223\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data=train_data.drop(train_data.index[[4415, 4400, 4399,4403,4397,4396, 4394,4414, 4393,4392,4404,4407,4420,\n",
    "                                             4412,4408,4391,4405,6840,6834,6837,6841,6816,6828,6831,601,576,584,608,\n",
    "                                             606,603,592,604,591, 587,3913,3914,3936,3921,3941,3937,3938,3136,3133,\n",
    "                                             3930,3933,3924,3917,246,270,266,259,253,251,250,271,6119,6122,6123,6131,\n",
    "                                             6160,6166,6167,6172,6212,6221,6230,6091,6108,7435,7460,7464,7466,7469,\n",
    "                                             7475,7489,7495,7500,7525,7552,7572,7591,7599]])\n",
    "\n",
    "# Class distribution\n",
    "print(train_data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090,),\n",
       " (6090,),\n",
       "                                           text\n",
       " target                                        \n",
       " 1       Forest fire near La Ronge Sask. Canada\n",
       " 0                                          NaN\n",
       " 1       Forest fire near La Ronge Sask. Canada\n",
       " 1       Forest fire near La Ronge Sask. Canada\n",
       " 0                                          NaN)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "\n",
    "train_data = pd.DataFrame(x_train, y_train)\n",
    "\n",
    "x_train.shape, y_train.shape, train_data.iloc[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucJtXeAaBw27"
   },
   "source": [
    "Let's have a look at the first texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xoEqv0tYBw27",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Courageous and honest analysis of need to use Atomic Bomb in 1945. #Hiroshima70 Japanese military refused surrender. https://t.co/VhmtyTptGR',\n",
       "       '@ZachZaidman @670TheScore wld b a shame if that golf cart became engulfed in flames. #boycottBears',\n",
       "       \"Tell @BarackObama to rescind medals of 'honor' given to US soldiers at the Massacre of Wounded Knee. SIGN NOW &amp; RT! https://t.co/u4r8dRiuAc\",\n",
       "       'Worried about how the CA drought might affect you? Extreme Weather: Does it Dampen Our Economy? http://t.co/fDzzuMyW8i',\n",
       "       '@YoungHeroesID Lava Blast &amp; Power Red #PantherAttack @JamilAzzaini @alifaditha'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USWw5FrJBw27"
   },
   "source": [
    "Certaines données sont mal étiquetées, nous allons les éliminer du Dataset (Pour plus de détails sur ces tweets, consultez les [travaux de Dmitri Kalyaevs](https://www.kaggle.com/code/dmitri9149/transformer-svm-semantically-identical-tweets/notebook))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "H0mc_pBZBw27"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6840 is out of bounds for axis 0 with size 6090",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_data=train_data.drop(\u001b[43mtrain_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4415\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4399\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4403\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4397\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4396\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4394\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4414\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4393\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4392\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4404\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4407\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4420\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                             \u001b[49m\u001b[32;43m4412\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4408\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4391\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4405\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6840\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6834\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6837\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6841\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6816\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6828\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6831\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m601\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m576\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m584\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m608\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                             \u001b[49m\u001b[32;43m606\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m603\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m592\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m604\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m591\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m587\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3913\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3914\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3936\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3921\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3941\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3937\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3938\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3136\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3133\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                             \u001b[49m\u001b[32;43m3930\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3933\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3924\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3917\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m246\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m270\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m266\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m259\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m253\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m251\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m271\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6119\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6122\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6123\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6131\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                             \u001b[49m\u001b[32;43m6160\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6166\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6167\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6172\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6212\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6221\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6230\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6091\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6108\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7435\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7460\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7464\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7466\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7469\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                             \u001b[49m\u001b[32;43m7475\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7489\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7495\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7525\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7552\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7572\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7591\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7599\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Class distribution\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_data.target.value_counts())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deep-learning-env/lib/python3.13/site-packages/pandas/core/indexes/base.py:5428\u001b[39m, in \u001b[36mIndex.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   5419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   5420\u001b[39m             warnings.warn(\n\u001b[32m   5421\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUsing a boolean indexer with length 0 on an Index with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5422\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlength greater than 0 is deprecated and will raise in a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5425\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   5426\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m5428\u001b[39m result = \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5429\u001b[39m \u001b[38;5;66;03m# Because we ruled out integer above, we always get an arraylike here\u001b[39;00m\n\u001b[32m   5430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mIndexError\u001b[39m: index 6840 is out of bounds for axis 0 with size 6090"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XUkh0h0Bw28"
   },
   "source": [
    "### Cleaning tweets\n",
    "\n",
    "As with all datasets, natural language data such as tweets require a great deal of cleaning. This step is part of what we call pre-processing. What do you think tweets need to be cleaned up? What elements need to be removed so that tweets are ready for the training phase?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "\n",
    "First of all, we're going to set up the pipelines: preparing the libraries and tools for cleaning up tweets. We're using The _NLTK_ , or Natural Language Toolkit, which is one of the most powerful natural language processing libraries, designed for symbolic and statistical natural language processing in Python. We use it for tokenization, stemming, lemmatization and stopword loading. We'll need to download a few pre-built tokenizer databases for this purpose ([Punkt](https://www.nltk.org/api/nltk.tokenize.punkt.html) and OMW...).\n",
    "\n",
    "Here's what our pipeline looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P0yt-OtBw28"
   },
   "outputs": [],
   "source": [
    "# Preparing libraries and tools for cleaning up tweets\n",
    "# Regular expressions\n",
    "import re\n",
    "# Punctuactions\n",
    "import string\n",
    "\n",
    "#Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "# Load stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Load Stemming, which consists in reducing a word to its \"root\" form\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW5Tvf-UBw28"
   },
   "source": [
    "We're going to start by writing a function to remove blank spaces, numbers, replace uppercase characters with lowercase ones, and remove special characters. To do this, we'll use _RegEx_ or regular expression, which is a sequence of characters that forms a search pattern. For more details, see https://www.w3schools.com/python/python_regex.asp or https://docs.python.org/fr/3/howto/regex.html\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcaFuie1Bw28"
   },
   "outputs": [],
   "source": [
    "# Function for cleaning each document: nlp_pipeline\n",
    "# Tweet= tweet corpus = document\n",
    "# A RegEx, or regular expression, is a sequence of characters that forms a search pattern.\n",
    "def nlp_pipeline(text):\n",
    "    # Convert uppercase letters to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace new line with a space\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Remove uppercase letters, all strings that are not letters or numbers\n",
    "    #PLEASE COMPLETE\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"(\\s\\-|-$)\", \"\", text)\n",
    "    #PLEASE COMPLETE\n",
    "    text = re.sub(r\"\\x89û\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqzpwt87Bw28"
   },
   "source": [
    "One of the most important steps in cleaning up tweets is to remove URLs and HTML tags, and then use _stemming_ to remove the end of the word and keep only the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkOjjwruBw28"
   },
   "outputs": [],
   "source": [
    "# Remove https\n",
    "def remove_url(sentence):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', sentence)\n",
    "\n",
    "def remove_html(sentence):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', sentence)\n",
    "\n",
    "#Stemming: stemming deletes the end of the word, leaving only the root.\n",
    "# Example: \"find\" becomes \"find\".\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_words(sentence):\n",
    "    words = sentence.split()\n",
    "    words = [stemmer.stem(word) for word in words ]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply stemming to the word \"fired\".\n",
    "print(stemmer.stem('fired'))\n",
    "\n",
    "# Apply stemming to word \"emergency\n",
    "print(stemmer.stem('emergency'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOaV2wuxBw28"
   },
   "source": [
    "Use the [stopword] module doc (https://pythonspot.com/nltk-stop-words/) to find these words in the English language, then delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgAk8EMmBw28"
   },
   "outputs": [],
   "source": [
    "mots_vides=  #PLEASE COMPLETE\n",
    "print('\\n')\n",
    "print(mots_vides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76_U8lrjBw29"
   },
   "outputs": [],
   "source": [
    "# Function that removes stopwords: words that are very common in the language studied but don't make sense.\n",
    "# as in French, the words: et, à,le, la, etc... (https://pythonspot.com/nltk-stop-words/ )\n",
    "mots_vides=stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    words = sentence.split()\n",
    "    words = #PLEASE COMPLETE\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD5dJj4GBw29"
   },
   "source": [
    "Observe punctuation using [string](https://docs.python.org/3/library/string.html) and then delete it. To do this, we recommend you search on the word _punctuation_ by keyword on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yrqTWU8Bw29"
   },
   "outputs": [],
   "source": [
    "punctuations = #PLEASE COMPLETE\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmZawA67C9GQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68HyPrj5Bw29"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def remove_punctuation(sentence):\n",
    "    #PLEASE COMPLETE\n",
    "\n",
    "    words = #PLEASE COMPLETE\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTSgbOE7Bw29"
   },
   "source": [
    "Delete emojis used in tweets.\n",
    "\n",
    "Note that the different types of pictographic elements are often contiguous in the UTF encoding, and we should be able to take advantage of this in RegEx. Each emoji has a unique Unicode assigned to it. When using Unicode with Python, replace \"+\" with the Unicode \"000\". Then prefix the Unicode with \"\\\". For example, U+1F605 will be used as \\U0001F605. Here, \"+\" is replaced by \"000\" and \"\\\" is prefixed by Unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgPfzR2FBw29"
   },
   "outputs": [],
   "source": [
    "# Remove emojis\n",
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                # emoticons\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                               # symbols & pictographs\n",
    "                           #PLEASE COMPLETE\n",
    "                                # transport & map symbols\n",
    "                           #PLEASE COMPLETE\n",
    "                               # flags (iOS)\n",
    "                           #PLEASE COMPLETE\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euxEsi0xBw29"
   },
   "source": [
    "\n",
    "Write a function to remove endings and isolate the canonical form of the word, also known as the lemma, which is often its [radical](https://www.espacefrancais.com/radicaux-prefixes-et-suffixes/), but not systematically. Particularly in the case of verbs, which must be passed to the infinitive. For example, the word \"find\" accepts the lemma \"find\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un13TjCNBw29"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "def lem_word(sentence):\n",
    "    words = sentence.split()\n",
    "    words = #PLEASE COMPLETE\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjJZ25oYBw2-"
   },
   "source": [
    "Sometimes, words less than or equal to two characters in length don't provide important information and it's better to remove them.\n",
    "\n",
    "Write the function that removes words with less than two characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA-HToOVBw2-"
   },
   "outputs": [],
   "source": [
    "# Remove words with less than two characters\n",
    "def remove_small(sentence):\n",
    "\n",
    "    words = sentence.split()\n",
    "    words = #PLEASE COMPLETE\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-ANvcXqBw2-"
   },
   "source": [
    "Let's write the _clean_text_ function that joins all these different functions to clean up tweets cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t-ZllN2Bw2-"
   },
   "outputs": [],
   "source": [
    " def clean_text(data):\n",
    "    data['text'] = data['text'].apply(lambda x : remove_url(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_html(x))\n",
    "    #data['text'] = data['text'].apply(lambda x : stem_words(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_punctuation(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_stopwords(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_emoji(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_small(x))\n",
    "    data['text'] = data['text'].apply(lambda x : lem_word(x))\n",
    "    data['text'] = data['text'].apply(lambda x : nlp_pipeline(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBhCQoDhBw2-"
   },
   "source": [
    "Now that the pre-processing pipeline is ready, let's apply it to the two types of data: the training dataset and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFWjZN_iBw2-"
   },
   "outputs": [],
   "source": [
    "# Apply cleaning to both types of data: the training data set and the test data set\n",
    "D = clean_text(train_data)\n",
    "test_data_c=clean_text(test_data)\n",
    "\n",
    "# Data before cleaning\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTGoNOOrBw2-"
   },
   "source": [
    "Check for empty tweets after cleaning and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtybQA8NBw2_"
   },
   "outputs": [],
   "source": [
    "print(\\\n",
    "      #PLEASE COMPLETE\n",
    "\n",
    "# Eliminate empty tweets if any.\n",
    "D = #PLEASE COMPLETE\n",
    "\n",
    "print(D.shape)\n",
    "\n",
    "D.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbV4QDsQBw2_"
   },
   "source": [
    "\n",
    "We can visually identify certain terms that are most often associated with our topic of interest, which in this case is \"disaster\". This can be done using a word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVrtRitXBw2_"
   },
   "outputs": [],
   "source": [
    "# You need to install wordCloud\n",
    "#!pip install wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create Wordcloud from tweets\n",
    "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "D.text\n",
    "all_words = ' '.join([text for text in D.text])\n",
    "wordcloud = #PLEASE COMPLETE\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlxG3HK-Bw3H"
   },
   "source": [
    "\n",
    "We're going to split the training data set of tweets into two parts: training and validation.\n",
    "As a reminder, what will each part be used for&nbsp;?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWwAdqFTBw3H"
   },
   "outputs": [],
   "source": [
    "# Learning-testing partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dtest = #PLEASE COMPLETE\n",
    "\n",
    "# Checking the split\n",
    "print(dtrain.shape)\n",
    "print(dtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVajWi1-Bw3H"
   },
   "source": [
    "The output shows a total of 7530 tweets, 5648 of which belong to training and 1883 to validation. Now that the dataset is ready, it's time to build the dictionary from the training sample.\n",
    "\n",
    "### Tokenization or segmentation  \n",
    "\n",
    "A model won't understand what to do with a string representing a sentence. Instead, it needs to be converted into an array of numbers representing the words in the sentence. A tokenizer should come in handy. How does it work?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "    \n",
    "To find out more about the various Tokenizer modules, see [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXYSp4c2Bw3H"
   },
   "outputs": [],
   "source": [
    "# Tokenization with Keras\n",
    "from keras. preprocessing.text import Tokenizer\n",
    "\n",
    "def define_tokenizer(train_sentences, val_sentences, test_sentences):\n",
    "    sentences = pd.concat([train_sentences, val_sentences, test_sentences])\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "    ## Creation of the dictionary from the sample documents\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def encode(sentences, tokenizer):\n",
    "    encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    encoded_sentences = #PLEASE COMPLETE\n",
    "\n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqQm_ZgzBw3H"
   },
   "source": [
    "What is the role of the <code class=\"cm-s-ipython language-python\"><span class=\"cm-variable\">padding</span><span class=\"cm-operator\">=</span><span class=\"cm-variable\">post</span></code>) option in the second function?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "\n",
    "Apply tokenization to the 3 data types, use the Tokenizer and encode the phrases in an array of index numbers representing the phrase. Name them respectively: `encoded_sentences`, `val_encoded_sentences` and `encoded_test_sentences` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwZg9BqeBw3I"
   },
   "outputs": [],
   "source": [
    "tokenizer = #PLEASE COMPLETE\n",
    "\n",
    "encoded_sentences = encode(dtrain['text'], tokenizer)\n",
    "val_encoded_sentences = #PLEASE COMPLETE\n",
    "encoded_test_sentences = #PLEASE COMPLETE\n",
    "\n",
    "# Number of documents processed\n",
    "print(tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yRSx0hHBw3I"
   },
   "source": [
    "The tokenizer provides some interesting information about the phrases it encodes. To get the index number assigned to a word, with `word_index`, we can look up the word in the Tokenizer's word index (which is just a Python dictionary with the words as keys and the index numbers as values), and look at some other information too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G9JXHyBBw3I"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.word_index['disaster'])\n",
    "print(tokenizer.word_index['target'])\n",
    "\n",
    "# Vocabulary size\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "# List of words and their frequencies\n",
    "print(list(tokenizer.word_counts.items())[:10])\n",
    "\n",
    "#List sorted in order of decreasing frequency\n",
    "print(sorted(list(tokenizer.word_counts.items()),key=lambda x: -x[1])[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAjruDs2Bw3I"
   },
   "source": [
    "The most frequently used important term is <em>like</em>, appearing 487 times in one or more documents. The term <em>fire</em> appears 484 times, <em>emergency</em> appears 226 times, <em>disaster</em> appears 220 times.\n",
    "\n",
    "Right, let's move on to the document matrix (text to matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-kNaRMvBw3J"
   },
   "source": [
    "### Word Embedding: convert a document into a matrix of numbers\n",
    "\n",
    "After preprocessing the text data and creating the dictionary, we need to do some <em>Word Embedding</em>.\n",
    "\n",
    "Why do we need to use Word Embedding in NLP?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "   \n",
    "There are various techniques available, depending on the model use case and dataset. We cite One Hot Encoding, TF-IDF, Word2Vec and FastText (https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08). We have chosen [GloVe](https://datamahadev.com/nlp-stanfords-glove-for-word-embedding/) for this task. GloVe (<em>Global Vectors for Word Representation</em>), was created by Stanford University. As its name suggests, it helps preserve global contexts, as it creates a global co-occurrence matrix by estimating the probability that a given word is co-occurring with other words. It therefore handles tasks requiring analogical reasoning about words and tasks requiring the capture of word similarity. It has predefined dense vectors for around 6 billion words in English literature, as well as many other general-purpose characters such as commas, braces and semicolons.\n",
    "\n",
    "Once we've pre-processed the text data and created the dictionary, we need to go through the Glove file of a specific dimension and compare each word with all the words in the dictionary, and if there's a match, copy the equivalent vector from the Glove and paste it into `embedding_matrix` at the corresponding index. The first thing to do, then, is to load the embedding.\n",
    "\n",
    "First, we'll download and decompress the GloVe embeddings. Specifically, we're going to retrieve the [Glove pre-entrapped representation](http://nlp.stanford.edu/data/glove.6B.zip). Please take your time to explore this data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXhvhy4lBw3J"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zf = ZipFile('glove.6B.zip', 'r')\n",
    "zf.extractall('glove.6B')\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PneIt3IiBw3J"
   },
   "source": [
    "Now let's compute an index that maps words to known embeddings, by analyzing the database of pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVSTyJWIBw3J"
   },
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "f=open(os.path.join('glove.6B', 'glove.6B.100d.txt'),'r',encoding='utf-8')\n",
    "for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:],'float32')\n",
    "        # Transform each word into a vector of dimension 100.\n",
    "        #PLEASE COMPLETE\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Check number of terms\n",
    "print('Found %s word vectors.' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNV0Rb9mBw3J"
   },
   "source": [
    "Testons la présence de certains termes, et leur similarité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po3i7bbYBw3J"
   },
   "outputs": [],
   "source": [
    "# Coordinates of the terms good and nice\n",
    "print(embedding_dict['good'])\n",
    "print(embedding_dict['nice'])\n",
    "\n",
    "# Similarity between good and nice: if the value is close to 1, then there's a strong similarity\n",
    "import scipy\n",
    "\n",
    "from scipy.spatial import distance\n",
    "print(1.0-scipy.spatial.distance.cosine(embedding_dict['good'],embedding_dict['nice']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFLwXlTHBw3K"
   },
   "source": [
    "We'll use the function below to ensure that the term we're looking for in our dictionary is present in GloVe's pre-trained representation. At this point, we can use our `embedding_dict` dictionary and our `word_dict` to calculate our embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXzBr5YwBw3K"
   },
   "outputs": [],
   "source": [
    "# Initial emmbedding matrix for our dataset\n",
    "hit=0\n",
    "misses=[]\n",
    "\n",
    "# Number of tokens ( numpy is zero-based)\n",
    "num_words = #PLEASE COMPLETE\n",
    "\n",
    "# Dimension of representation =100 according to Glove chosen\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "# Fill the matrix with the coordinates from the pre-trained representation\n",
    "# Provided that the dictionary term we're looking for is present in GloVe's pre-trained representation\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "\n",
    "    emb_vec = embedding_dict.get(word)\n",
    "\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec\n",
    "        hit = hit+1\n",
    "    else:\n",
    "        misses.append(word)\n",
    "\n",
    "# Control display: the number of terms found and not found in GloVe's pre-trained representation.\n",
    "\n",
    "print( #PLEASE COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT2QsXlOBw3K"
   },
   "source": [
    "We find 6009 non-referenced terms, i.e. 6009 terms present in the tweets but not in GloVe. The corresponding rows are transformed into zero in our matrix. This is a loss of information, as they are never trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzjdAnzABw3K"
   },
   "outputs": [],
   "source": [
    "print(misses[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpMIYT9-Bw3K"
   },
   "source": [
    "We can check that these terms are not present in the chosen Glove document, as there are terms that have been incorrectly transformed in the pre-processing phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9GmkryqBw3K"
   },
   "source": [
    "### Preparing the pipeline\n",
    "\n",
    "With the sentences encoded, they can now be prepared for input into the model. TensorFlow provides an API for formatting data in its own format. Although data can be inserted in a more common format (such as numpy arrays), TensorFlow seems to prefer its own format and provides some handy features as incentives.\n",
    "\n",
    "As a first step, therefore, we're going to convert coded sentences and labels into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21deKnUrBw3K"
   },
   "outputs": [],
   "source": [
    "tf_data = tf.data.Dataset.from_tensor_slices((encoded_sentences, dtrain['target'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkGDW_1UBw3L"
   },
   "source": [
    "Now that the data is in TensorFlow format, a few practical methods can be added to improve training. These include shuffling the data at each stage of training, processing the next batch of data for training while the current batch of data is being trained, and defining each batch as a padded batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME4wfXkxBw3L"
   },
   "outputs": [],
   "source": [
    "def pipeline(tf_data, buffer_size=100, batch_size=32):\n",
    "    tf_data = tf_data.shuffle(buffer_size)\n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    tf_data = #PLEASE COMPLETE\n",
    "\n",
    "    return tf_data\n",
    "\n",
    "tf_data = pipeline(tf_data, buffer_size=1000, batch_size=32)\n",
    "\n",
    "print(tf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1wJQiDyBw3L"
   },
   "source": [
    "Let's define a similar pipeline for the test dataset. The difference is the absence of shuffling to speed up validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkoKxOj-Bw3L"
   },
   "outputs": [],
   "source": [
    "tf_val_data = #PLEASE COMPLETE\n",
    "\n",
    "def val_pipeline(tf_data, batch_size=1):\n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    tf_data = #PLEASE COMPLETE\n",
    "\n",
    "    return tf_data\n",
    "\n",
    "tf_val_data = val_pipeline(tf_val_data, batch_size=len(dtest))\n",
    "\n",
    "print(tf_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W67wF2N1Bw3L"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "We're now ready to define and train the model. First, let's define the model!!!!\n",
    "\n",
    "![graphRNN1.png](attachment:graphRNN1.png)\n",
    "\n",
    "Three layers need to be defined: an embedding layer, an RNN layer and a dense layer.\n",
    "Explain how each layer works.\n",
    "\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HZNbiG0Bw3L"
   },
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    len(tokenizer.word_index)+1,\n",
    "    100,\n",
    "    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False # this embedding layer is fixed, it must not be\n",
    "                    # it must not be included in the training layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE_b9Tf0Bw3L"
   },
   "source": [
    "### RNN: Simple recurrent neural networks\n",
    "\n",
    "Simple recurrent neural networks are not suitable for natural language data, and are more commonly used for sequential data.\n",
    "For the RNN layer, simple RNNs don't perform well; indeed, one of their main problems is vanishing gradients. RNNs can be quite long, and may have difficulty backpropagating gradients to the first layer of the network. When this happens, the network cannot learn the relationships between distant tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYxk2pEOBw3M"
   },
   "source": [
    "What do you think is the consequence of the Vanishing problem?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy1zzP3XBw3M"
   },
   "source": [
    "If you feel like it, you can try out simple RNNs and see how they work in our case. Don't do it during the Workshop, as you'll be wasting learning time for nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z-wr655Bw3M"
   },
   "source": [
    "How can you avoid this problem?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "\n",
    "Which of the three RNN architectures can do this?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "\n",
    "We're going to apply and compare these types of recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o93uZS3vBw3M"
   },
   "source": [
    "### GRU: Gated Recurrent Units\n",
    "\n",
    "First, we'll try to understand the architecture of GRU (Gated Recurrent Unit) neural networks.\n",
    "These neural networks are made up of two gates:\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Each \"gate\" corresponds to a small neural network with a sigmoid activation function, the aim of which is to bring the values of the gate's input vectors between 0 and 1.\n",
    "\n",
    "What are the names of the two gates (A and B in the figure)? How do they work?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "\n",
    "A more detailed description of GRU can be found in [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).\n",
    "\n",
    "So let's create Model 1 with the GRU type, consisting of a hidden layer, 128 neurons, and a droupout of 0.2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scVplZWEBw3M"
   },
   "outputs": [],
   "source": [
    "# Creation of model 1 architecture\n",
    "model1 = tf.keras.Sequential([\n",
    "    embedding,\n",
    "   tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    # GRU with 128 neurons and dropout=0.2\n",
    "    #PLEASE COMPLETE\n",
    "    ,tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC4kyp6LBw3M"
   },
   "source": [
    "Then we compile the model by defining the learning function (with an Adam of 0.01) and the loss function (here, loss binary crossentropy). We have also added a metric parameter so that the model's accuracy can be printed per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrJlBh1KBw3M"
   },
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    loss= #PLEASE COMPLETE\n",
    "    , optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp6G0wRuBw3M"
   },
   "source": [
    "Finally, we start training the model on the train part, with 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2oFLA5rBw3N"
   },
   "outputs": [],
   "source": [
    "history1 = model1.fit(\n",
    "    tf_data,\n",
    "    validation_data = tf_val_data,\n",
    "    epochs = 50\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rnp5MGrUBw3N"
   },
   "source": [
    "\n",
    "We'll visualize both metrics: error and accuracy, per epoch, during model training to get a better idea of how the training went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0eaNvJBBw3N"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(history1.history['loss'], label='train')\n",
    "axs[0].plot(history1.history['val_loss'], label='val')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('Accuracy')\n",
    "axs[1].plot(history1.history['accuracy'], label='train')\n",
    "axs[1].plot(history1.history['val_accuracy'], label='val')\n",
    "axs[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9T6pFIOBw3N"
   },
   "source": [
    "\n",
    "### LSTM\n",
    "\n",
    "We're going to test another model using the neural network type LSTM. The use of LSTM is very effective for performing NLP tasks.\n",
    "\n",
    "An LSTM network is organized similarly to an RNN, but two states are transmitted from one layer to the next: the real state and the hidden vector. At each unit, the hidden vector is combined with the input, and together they control what happens to the state and output via gates. Each gate has a sigmoid activation (output in range), which can be seen as a bit-by-bit mask when multiplied by the state vector. LSTMs then have all three gates.\n",
    "\n",
    "Note that the horizontal line in the following diagram corresponds to the almost identical propagation of the cell state vector (_cell state_), which ensures the propagation of the initial information.\n",
    "\n",
    "![LSTM1.jpg](attachment:LSTM1.jpg)\n",
    "\n",
    "What is the name and function of these different gates?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "\n",
    "More detailed explanations of this architecture can be found [here](https://larevueia.fr/quest-ce-quun-reseau-lstm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH0ddr9dBw3N"
   },
   "source": [
    "For scenarios that require random access to the input sequence, it makes more sense to run the recurrent calculation in both directions. RNNs that enable two-way computation are called <em>bidirectional</em> RNNs, and they can be created by wrapping the recurrent layer with a special bidirectonal layer.\n",
    "\n",
    "The bidirectional layer makes two copies of the layer it contains, and sets the `go_backwards` property of one of these copies to <code class=\"cm-s-ipython language-python\"><span class=\"cm-keyword\">True</span></code>, making it go in the opposite direction along the sequence.\n",
    "Recurrent networks, unidirectional or bidirectional, capture patterns in a sequence, and store them in state vectors or return them as output. As with convolutional networks, we can build another recurrent layer after the first to capture higher-level patterns, built from lower-level patterns extracted by the first layer. This leads us to the notion of a multi-layer RNN, which consists of two or more recurrent networks, where the output of the previous layer is passed on to the next layer as input.\n",
    "![image.png](attachment:image.png)\n",
    "                                Figure by Fernando López.\n",
    "\n",
    "Let's build a bidirectional single-hidden-layer LSTM with 128 neurons and dropout equal to 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI3UyHLlBw3N"
   },
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    embedding,\n",
    "    tf.keras.layers.SpatialDropout1D(0.4),\n",
    "    # bi-directional single hidden layer LSTM with 128 neurons and dropout equal to 0.3\n",
    "    #PLEASE COMPLETE\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.compile(\n",
    "    # loss function binary cross entropy\n",
    "    loss= #PLEASE COMPLETE\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    # accuracy metric\n",
    "    #PLEASE COMPLETE\n",
    "\n",
    "history2 = model2.fit( tf_data, validation_data = tf_val_data, epochs = 50 )\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(history2.history['loss'], label='train')\n",
    "axs[0].plot(history2.history['val_loss'], label='val')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('Accuracy')\n",
    "# Accuracy representation for axis 2 data train\n",
    "#PLEASE COMPLETE\n",
    "# Accuracy representation for data val in axis 2\n",
    "#PLEASE COMPLETE\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75QoXbyBw3N"
   },
   "source": [
    "\n",
    "Compare the results found by LSTM with those of GRU. What can we conclude?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX_tfO8IBw3N"
   },
   "source": [
    "## Model performance evaluation\n",
    "\n",
    "We now know which architecture is more suitable, but it's interesting to understand why. Let's see which sentences our two models misinterpreted. To do this, the models need to produce predictions for the dataset. This requires a slightly different pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgzxQBGiBw3O"
   },
   "outputs": [],
   "source": [
    "prediction1 = model1.predict(tf_val_data)\n",
    "prediction1 = np.concatenate(prediction1).round().astype(int)\n",
    "dtest['prediction1'] = prediction1\n",
    "\n",
    "prediction2 = model2.predict(tf_val_data)\n",
    "prediction2 = np.concatenate(prediction2).round().astype(int)\n",
    "dtest['prediction2'] = prediction2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8O3o3XGBw3O"
   },
   "source": [
    "### Model evaluation using the test dataset\n",
    "\n",
    "Let's take a look at the results of Model 1 and Model 2 to get an idea of their performance. The quickest and simplest method of evaluation is to examine the metrics produced by the model. The final metrics can be extracted using the `evaluate` method on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFhS5lIJBw3O"
   },
   "outputs": [],
   "source": [
    "# GRU model evaluation on the test part\n",
    "score1=model1.evaluate(dtest['prediction1'],dtest['target'],verbose=1)\n",
    "\n",
    "print(\"Validation Score model1:\", score1[0])\n",
    "print(\"Validation Accuracy model1:\", score1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtmguGkvBw3O"
   },
   "source": [
    "Display the evaluation of the LSTM model on the test part. We need the validation error value as well as the accuracy or precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PgVreDtBw3O"
   },
   "outputs": [],
   "source": [
    "# Evaluation of the LSTM model on the test part\n",
    "score2 = #PLEASE COMPLETE\n",
    "\n",
    "# Display error: Validation Score model1 and accuracy of model 2: Validation Accuracy model2\n",
    "print( #PLEASE COMPLETE\n",
    "print( #PLEASE COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8mdFKH1Bw3O"
   },
   "source": [
    "What do you think of these values? What do you conclude?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kArCFEEeBw3O"
   },
   "source": [
    "Let's now try to go into a little more detail about the performance of this architecture.\n",
    "\n",
    "### Evaluation of LSTM results\n",
    "Let's start by looking at the false positives. What situation do these false positives correspond to?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>\n",
    "So let's count these false positives, and take a look at the first 10 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrCqkSE-Bw3O"
   },
   "outputs": [],
   "source": [
    "false_positives = #PLEASE COMPLETE\n",
    "\n",
    "print('Count of false positives: ' + str(len(false_positives)))\n",
    "\n",
    "false_positives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov0-FX16Bw3P"
   },
   "source": [
    "Then do the same with false negatives. What network behavior do they correspond to?\n",
    "\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6boRaKCBw3P"
   },
   "outputs": [],
   "source": [
    "false_negatives = #PLEASE COMPLETE\n",
    "\n",
    "print('Count of false negatives: ' + str(len(false_negatives)))\n",
    "\n",
    "false_positives.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oUNYJ_yBw3P"
   },
   "source": [
    "Once the model is trained, there are just a few steps left to load the test data and use the model to label test sentences as disaster or not. First, convert the data into a TensorFlow dataset and apply the pipeline methods. The pipeline has been adjusted slightly to take account of the fact that we don't want shuffling and the different shape of the input (without labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmEO2MgLBw3P"
   },
   "outputs": [],
   "source": [
    "tf_test_data = tf.data.Dataset.from_tensor_slices((encoded_test_sentences))\n",
    "\n",
    "\n",
    "def test_pipeline(tf_data, batch_size=1):\n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    tf_data = tf_data.padded_batch(batch_size, padded_shapes=([None]))\n",
    "\n",
    "    return tf_data\n",
    "\n",
    "tf_test_data = test_pipeline(tf_test_data)\n",
    "\n",
    "print(len(tf_test_data))\n",
    "\n",
    "# Use the template to label phrases in test data as disastrous or not\n",
    "predictions = model2.predict(tf_test_data)\n",
    "\n",
    "predictions = np.concatenate(predictions).round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLKJcGWMBw3P"
   },
   "source": [
    "Submit the sentences from the test dataset and save the submission in a CSV file so that you can classify each tweet with a disaster evoked or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B9Kd6wKBw3P"
   },
   "outputs": [],
   "source": [
    "\n",
    "submission = #PLEASE COMPLETE\n",
    "submission.index = #PLEASE COMPLETE\n",
    "submission.to_csv('submission.csv')\n",
    "\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpPFcjwUBw3P"
   },
   "source": [
    "As a result of this study, we were able to predict the sentiment analysis of tweets by RNN.\n",
    "\n",
    "\n",
    "### Going further with NLP\n",
    "\n",
    "The training, validation and test datasets are likely to contain words that the other datasets do not. If the model is only trained on the words in the training dataset, there may be a problem of overfitting when the model tries to read words it doesn't recognize in the validation and test datasets.\n",
    "\n",
    "The question is _how much of a problem is this?\n",
    "The function below takes two datasets and counts how words match and don't match, it will give us an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tzAAmGVBw3Q"
   },
   "outputs": [],
   "source": [
    "def compare_words(train_words, test_words):\n",
    "    unique_words = len(np.union1d(train_words, test_words))\n",
    "    matching = #PLEASE COMPLETE\n",
    "    not_in_train = len(np.setdiff1d(test_words, train_words))\n",
    "    not_in_test = len(np.setdiff1d(train_words, test_words))\n",
    "\n",
    "    print('Number of words in both parts dtrain and dtest: ' + str(unique_words))\n",
    "    print('Number of words in matching: ' + str(matching))\n",
    "    print('Number of words in the dtrain dataset and not in the test dataset: ' + str(not_in_test))\n",
    "    print('Number of words in the test dataset and not in the dtrain dataset: ' + str(not_in_train))\n",
    "\n",
    "# Comparison between training data and validation data\n",
    "compare_words(encoded_sentences, val_encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvoHcRpNBw3Q"
   },
   "source": [
    "This shows that only 25% of words are found in both datasets, and that the dataset used for validation contains 16% of words not found in the dataset used for training. Clearly, this adversely affects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWArKDhBBw3Q"
   },
   "outputs": [],
   "source": [
    "# Comparison of training and test data\n",
    "compare_words(encoded_sentences, encoded_test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLPS1kH6Bw3Q"
   },
   "source": [
    "28% are found in both datasets.\n",
    "47% are in the training dataset and not in the test dataset.\n",
    "25% are in the test dataset and not in the training dataset.\n",
    "\n",
    "We can use the lexical rule-based approach (https://datapeaker.com/fr/Big-Data/analyse-des-sentiments-bas%C3%A9e-sur-des-r%C3%A8gles-en-python-pour-les-scientifiques-des-donn%C3%A9es/). These simple approaches, widely used in NLP, include TextBlob, VADER and SentiWordNet. They look for opinion words in a text and then classify them according to the number of words announcing a disaster or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF4OUwzQBw3Q"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations, you've just completed your first analysis of natural language processing or NLP! You've analyzed Tweets using the GRU and LSTM recurrent neural networks.\n",
    "\n",
    "You've seen how to clean up corpora, visualize Twitters results and then you've been able to implement, train and evaluate the RNN as well as improve its performance by trying out GRU and LSTM. Congratulations!\n",
    "\n",
    "But you still need to improve the model's performance. First of all, the accuracy is only 79.3%. Obviously, increasing the size of the network will have a significant impact on learning speed. There are certainly other approaches to improving results. Here in the word embedding step, we used GloVe, an unsupervised learning algorithm that matches words in a space where semantic similarity between words is observed by the distance between words. Other word embedding techniques, such as Word2vec or Se2seq, can also be used. There are also preprocessed architectures, called Transformers, that you can use, such as BERT(Devlin et al., 2018), and GPT-2 (de Radford et al 2019). To learn more about these approaches, please visit this site [link](https://france.devoteam.com/paroles-dexperts/lstm-transformers-gpt-bert-guide-des-principales-techniques-en-nlp/)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Pièces jointes",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "789px",
    "left": "22px",
    "top": "189.125px",
    "width": "297.812px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
