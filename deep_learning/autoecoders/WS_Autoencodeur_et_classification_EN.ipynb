{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H365Wt7vSauN"
   },
   "source": [
    "# Convolutional Auto-encoder and Dimension Reduction\n",
    "\n",
    "\n",
    "![index.jpg](attachment:index.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXjnTQAnSauQ"
   },
   "source": [
    "This workshop deals with a new neural network architecture: auto-encoders. This architecture consists of two parts, the encoder and the decoder. It enables us to build a new, more compact representation of a dataset, with fewer descriptors, thus reducing the dataset's dimensionality. In this workshop, we'll take a look at how auto-encoders work on a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neNjzWyLSauR"
   },
   "source": [
    "# Dataset import and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tRmP5SDSauS"
   },
   "source": [
    "The dataset used in this Workshop is the famous MNIST dataset (handwritten digits). We use the CSV version of this dataset, available here: https://pjreddie.com/projects/mnist-in-csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTQX7roxSauS"
   },
   "source": [
    "We'll keep only the mnist_test.csv file, containing 10,000 grayscale images of size 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1K7Zb07SauT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-QwgdTWSauU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'mnist_test.csv', header=None)\n",
    "df['pixels'] = df.index.map(lambda x: np.array(df.iloc[x][1:]))\n",
    "dropcols = df.columns[(df.columns != 0) * (df.columns != 'pixels')]\n",
    "df.drop(dropcols, axis=1, inplace=True)\n",
    "df.columns = ['label','pixels']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8dpgiL1SauU"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSLMayEQSauV"
   },
   "source": [
    "We can start by displaying an example image from the MNIST dataset (Grayscale ==> Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peSbLgPVSauV"
   },
   "outputs": [],
   "source": [
    "fig = plt.imshow(df['pixels'][10].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqGp0ZVESauW"
   },
   "source": [
    "# Size reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuE0-LpoSauW"
   },
   "source": [
    "In this section, we'll look at how to distinguish between the features of different handwritten digits, using only the raw pixel values as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2CKpIMLSauX"
   },
   "source": [
    "Let's look at 3 numbers. Keep them as they are for now, and you'll have time to play around with them in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GpcZsseSauX"
   },
   "outputs": [],
   "source": [
    "## Figures considered ##\n",
    "labels = [1,6,8]\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "X = np.array([df['pixels'][i] for i in df.index if df['label'][i] in labels])\n",
    "y = #PLEASE COMPLETE\n",
    "\n",
    "print('X shape: '+str(X.shape))\n",
    "print('y shape: '+str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63dnTHYwSauX"
   },
   "source": [
    "We have 28 x 28 = 784 features. This is a high dimension (~ same order of magnitude as the number of points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkZSZTI_SauY"
   },
   "source": [
    "In this case, we can't rely on univariate analysis: it's obvious that the value of a given pixel in the image won't, on its own, determine the number that is written (the class). We need to study the relationship between pixel values: let's see how dimensionality reduction methods can help us distinguish noise, correlation and information patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi4uQkHsSauY"
   },
   "source": [
    "### Principal Component Analysis (PCA): Linear Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnu9EKXSSauY"
   },
   "source": [
    "As you saw last year in the IA block (ML Pipeline section), we can use PCA (Principal Component Analysis) to analyze and reduce image size. For more details on PCA, please consult the following links:\n",
    "* https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/\n",
    "* https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_PCA.pdf\n",
    "* https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-641j-introduction-to-neural-networks-spring-2005/lecture-notes/lec18_pca.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqAS8kcpSauZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGthpF2WSauZ"
   },
   "outputs": [],
   "source": [
    "## PCA calculation\n",
    "pca = #PLEASE COMPLETE\n",
    "XPCA = #PLEASE COMPLETE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t1wTsjPSauZ"
   },
   "outputs": [],
   "source": [
    "XPCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQRZcmVMSauZ"
   },
   "outputs": [],
   "source": [
    "## Axis selection for visualization\n",
    "component_x = 1\n",
    "component_y = 2\n",
    "\n",
    "## Plotting\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(labels)):\n",
    "    ax.scatter(XPCA[y == labels[i],component_x-1],\n",
    "               XPCA[y == labels[i],component_y-1],\n",
    "               c=colors[i], label=labels[i], alpha=0.4)\n",
    "\n",
    "l = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpfNU3iuSauZ"
   },
   "source": [
    "It's also possible to display the data in 3D to show an extra dimension of our PCA-generated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ssm5KZ2SSaua"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "## Choice of axes for visualization\n",
    "component_x = 1\n",
    "component_y = 2\n",
    "component_z = 3\n",
    "\n",
    "## Plotting\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "for i in range(len(labels)):\n",
    "    ax.scatter(XPCA[y == labels[i],  component_x-1],\n",
    "               XPCA[y == labels[i],component_y-1],\n",
    "               XPCA[y == labels[i],component_z-1],\n",
    "               c=colors[i], label=labels[i], alpha=0.4)\n",
    "\n",
    "l = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdQZdE7rSaua"
   },
   "source": [
    "Try to visualize different axes and find those that do (or don't) distinguish classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgpn0jlnSaua"
   },
   "source": [
    "### T-distributed Stochastic Neighbor Embedding : Approche non-Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrCdqOvwSaua"
   },
   "source": [
    "Most data sets have a non-linear relationship between features and data points in a high-dimensional space. Therefore, we want to create a low-dimensional encoding for these data that preserves the relationship between the different points in the original space. To this end, the TSNE (T-distributed Stochastic Neighbor Embedding) algorithm is a probabilistic approach that places objects (e.g. images), described by high-dimensional vectors (e.g. grayscale values), in a low-dimensional space in such a way as to preserve the identity of the neighbors. For more details on this algorithm, please consult the following resources:\n",
    "* http://www.cs.columbia.edu/~verma/classes/uml/lec/uml_lec8_tsne.pdf\n",
    "* https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/lec13_handout.pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2_FlEltSaua"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMzjYUEXSaub"
   },
   "outputs": [],
   "source": [
    "## Calculation of t-SNE 2D projection\n",
    "\n",
    "## Parameters with a real influence on accuracy\n",
    "perplexity = 30\n",
    "learning_rate = 200\n",
    "n_iter = 1000\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter)\n",
    "XTSNE = #PLEASE COMPLETE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex26LY6SSaub"
   },
   "outputs": [],
   "source": [
    "## Plotting\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(labels)):\n",
    "    ax.scatter(XTSNE[y == labels[i],0],\n",
    "               XTSNE[y == labels[i],1],\n",
    "               c=colors[i], label=labels[i], alpha=0.4)\n",
    "l = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MozkhfTgSaub"
   },
   "source": [
    "# Impact on supervised model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhRIZcMUSaub"
   },
   "source": [
    "In this section, we consider a supervised machine learning model for predicting the class of an image (which digit it corresponds to). We assume that the classification task, as well as the algorithms for solving it, are skills acquired in the AI Block. For more information on this section, please refer to the Classification and ML Pipeline sections covered in 4th year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnsFzfDHSauc"
   },
   "source": [
    "\n",
    "We will study the impact of dimensionality reduction on 3 classifiers with different mechanisms:\n",
    "* Naive Bayes Classifier (NB)\n",
    "* SVM (SVM)\n",
    "* Random Forest Classifier (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqy2dnyHSauc"
   },
   "source": [
    "We can give them 3 different inputs as characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02zY9TvhSauc"
   },
   "source": [
    "* All pixel values (raw)\n",
    "* The first n components of the PCA (10 for example)\n",
    "* The 2 dimensions of the t-SNE projection (tsne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyAvEo45Sauc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1gv0YYISauc"
   },
   "outputs": [],
   "source": [
    "def fit_my_model(model, features, test_size):\n",
    "\n",
    "    ## Building train and test sets from all the  features\n",
    "    if features == 'raw':\n",
    "        X_train, X_test, y_train, y_test = #PLEASE COMPLETE\n",
    "    elif features == 'tsne':\n",
    "        X_train, X_test, y_train, y_test = #PLEASE COMPLETE\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = #PLEASE COMPLETE\n",
    "\n",
    "    print(\"Training samples: \"+str(X_train.shape[0]))\n",
    "    print(\"Testing samples: \"+str(X_test.shape[0]))\n",
    "    print(\"Number of features: \"+str(X_train.shape[1]))\n",
    "\n",
    "    ## Fit model\n",
    "    if model == 'nb':\n",
    "        clf = GaussianNB()\n",
    "    elif model == 'svm':\n",
    "        clf = SVC(gamma='auto')\n",
    "    elif model == 'rf':\n",
    "        clf = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=None, max_features=np.min([10, X_train.shape[1]]))\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Print scores\n",
    "    learningScore = clf.score(X_train, y_train)\n",
    "    generalizationScore = clf.score(X_test, y_test)\n",
    "    print('Learning score: '+str(learningScore))\n",
    "    print('Generalization score: '+str(generalizationScore))\n",
    "\n",
    "    return generalizationScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFyRY18vSaud"
   },
   "source": [
    "Now we choose the classification model and the dimension reduction method. The reduction will be the input to the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtR2wdEFSaud"
   },
   "outputs": [],
   "source": [
    "model = 'rf' ## svm, nb, rf\n",
    "features = 'raw' ## raw, tsne, or integer corresponding to the first n PCA components\n",
    "test_size = 0.2\n",
    "\n",
    "score = fit_my_model(model, features, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HMzzJVvSaul"
   },
   "source": [
    "Let's test all configurations and build a results table with test scores, to help you answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPij_eXNSaul"
   },
   "outputs": [],
   "source": [
    "comparison_table = pd.DataFrame(columns = ['raw', 'pca', 'tsne'], index=['svm', 'nb', 'rf'])\n",
    "for f in comparison_table.columns:\n",
    "    for m in comparison_table.index:\n",
    "        print(\"\\n\"+f + ' - ' + m)\n",
    "        comparison_table.loc[m,f] = fit_my_model(m, f if f!='pca' else 10, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KBPy9YsSaul"
   },
   "source": [
    "Let's visualize the result as a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDC9adSxSaul",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Heatmap with summarized results\n",
    "fig = sns.heatmap(comparison_table.astype('float'), annot=True, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxRuMgY-Saul"
   },
   "source": [
    "What is the effect of dimensionality reduction on the 3 classifiers? Try to explain why with your intuition...\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWNcsEZ6Saul"
   },
   "source": [
    "These examples only concern supervised classifiers. Can you imagine the impact on other tasks: regression, clustering, anomaly detection...?\n",
    "<em>PLEASE COMPLETE</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dal1R83QSaum"
   },
   "source": [
    "# Auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jku3JRXtSaum"
   },
   "source": [
    "In this section, we will build a step-by-step autoencoder architecture, train it and evaluate it on MNIST data. We'll start with a standard, fully-connected autoencoder, followed by a variational autoencoder in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4FXof1USaum"
   },
   "source": [
    "To prepare for this work, let's import the appropriate libraries and use the full Keras MNIST dataset, as neural networks will need more data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CL6sNI1xSaun"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig7CyxUGSaun"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yad7aRiNSaun"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train = np.array([x.flatten() for x in X_train]).astype(\"float32\") / 255.\n",
    "X_test = np.array([x.flatten() for x in X_test]).astype(\"float32\") / 255.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gNCHTOTSauo"
   },
   "source": [
    "It is customary to normalize the input data of a neural network between 0 and 1, as this speeds up the training of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3STxx06Sauo"
   },
   "source": [
    "### Standard auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcS1Rn2ASaup"
   },
   "source": [
    "In this first part, we implement a standard (fully connected) auto-encoder. Let's start with the separate encoder and decoder architectures. Pay attention to the activation of the final layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqqPo1-ISaup"
   },
   "source": [
    "In this section, we will build an Encoder including:\n",
    "* A 28x28 input data layer,\n",
    "* A Fully connected hidden layer of 200 units,\n",
    "* an output of 15 units (you can change the number), which represents the size of the latent space.\n",
    "\n",
    "![autoencoder-architecture.png](attachment:autoencoder-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI91HVT5Saup"
   },
   "outputs": [],
   "source": [
    "latent_space_dim = #PLEASE COMPLETE\n",
    "\n",
    "# Encoder architecture\n",
    "encoder_inputs = #PLEASE COMPLETE\n",
    "hidden1 = #PLEASE COMPLETE\n",
    "latent_space = #PLEASE COMPLETE\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, latent_space, name=\"encoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjZRoEVgSauq"
   },
   "source": [
    "You can even display and save the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6N_7Ie2Sauq"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(encoder, to_file='encodersimple.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYcH1HmwSauq"
   },
   "source": [
    "Now let's turn to the decoder. Applying the same principle, this cell is symmetrical to the previous one. The only difference is the last layer, where the activation function will use sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PnTzEvfSaur"
   },
   "outputs": [],
   "source": [
    "# Decoder architecture\n",
    "decoder_inputs = #PLEASE COMPLETE\n",
    "hidden2 = #PLEASE COMPLETE\n",
    "decoder_outputs = #PLEASE COMPLETE\n",
    "\n",
    "decoder = keras.Model(decoder_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2iN_uIOSaur"
   },
   "source": [
    "Here too, the model is displayed and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x940gDDnSaur"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(decoder, to_file='decodersimple.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oZ8fHqbSaur"
   },
   "source": [
    "Now we'll combine the two architectures (Encoder, Decoder) to build the complete AutoEncoder (AE). As a reminder, the encoder output is the decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkbNmQmYSaur"
   },
   "outputs": [],
   "source": [
    "# Combining the two Architecture (Enc,Dec)\n",
    "outputs = #PLEASE COMPLETE\n",
    "autoencoder = #PLEASE COMPLETE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF_2wmTSSaur"
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s30ufAAPSaus"
   },
   "source": [
    "# Note: it is possible to create an Autoencoder class to create a reusable architecture.  \n",
    "See https://www.tensorflow.org/tutorials/generative/autoencoder#first_example_basic_autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XYP5S7SSaus"
   },
   "source": [
    "We now need to define the loss function for training. As this is a classification task, and as you saw last year (Block IA - Classification), in the case of MNIST images, we can use a binary cross-entropy per pixel, summed over the whole image. In other words, we can use the binary cross-entropy function to estimate the reconstruction error on each pixel of the image. In total, we'll have 784 cross-entropy values for which we'll calculate an average. This average represents the total error. We can also use a root-mean-square error, but let's leave that as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqM82EPcSaus"
   },
   "outputs": [],
   "source": [
    "# Definition of the loss function (Loss Fct)\n",
    "reconstruction_loss = keras.losses.binary_crossentropy(encoder_inputs, outputs) * 784\n",
    "\n",
    "autoencoder_loss = K.mean(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfOdKSndSaus"
   },
   "source": [
    "All that remains is to add the loss function to the auto-encoder, and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIvNDrkXSaus"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "autoencoder.add_loss(autoencoder_loss)\n",
    "autoencoder.compile(optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNc0FVagSaut"
   },
   "source": [
    "Let's see what happens. Let's train the model over 30 epochs, dividing the dataset into batches of 128 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIk5OLE_Saut"
   },
   "outputs": [],
   "source": [
    "# Executing the model\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "          #PLEASE COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcd07nrDSaut"
   },
   "source": [
    "We'll save the weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvpFX3DBSaut"
   },
   "outputs": [],
   "source": [
    "autoencoder.save_weights('./autoencoder-77.5.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LimESIBMSaut"
   },
   "source": [
    "To get an idea of the performance of the model we've developed, we'll visualize the values of the Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie3oQ_xnSaut"
   },
   "outputs": [],
   "source": [
    "# Visualization of learning (Train) and validation (Test) losses\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvT5asvLSaut"
   },
   "source": [
    "We seem to be getting a pretty reasonable model! Let's have a look at the quality of the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nS8_XhpSaut"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # Original images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Reconstructed images\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    encoded_img = encoder.predict(np.array([X_test[i]]).reshape(1,784))\n",
    "    decoded_img = decoder.predict(encoded_img).reshape(28, 28)\n",
    "    plt.imshow(decoded_img, cmap=\"gray\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ultavd2ISaut"
   },
   "source": [
    "We can see that some details are lost, but the overall shapes are correctly reconstructed.  \n",
    "\n",
    ">The result of this process is that the auto-encoder outputs the same image as it receives as input, while passing through a **latent space** of smaller dimension: here 15.\n",
    ">This may not seem very useful, but as we've separated the auto-encoder into an encoder on the one hand and a decoder on the other, we can use them separately.  \n",
    ">For example, keeping only the encoder, we can represent a one-digit image made up of 784 pixels with a vector of 15 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ucy33p_BSauu"
   },
   "outputs": [],
   "source": [
    "x_test_latent = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A7hb-FCSauu"
   },
   "source": [
    "We've just designed a \"Generative\" model that can be used in a number of applications, for example:\n",
    "* Dimension reduction: as mentioned above, we can use the Encoder function (Encoder part of the network) to reduce the dimension of input data. For example, an image is given as input to the Encoder, which produces, according to the previous example, a vector containing 15 values (between 0 and 1).\n",
    "* Generating new images: we can also use the Decoder part of the network and feed it with vectors (15 values in the previous example) generated at random. By doing so, the Decoder will generate new images not contained in the Dataset.\n",
    "* Noise reduction (Denoising): we can also train the AutoEncoder with noisy input and denoised output images, enabling us to design a Denoising AutoEncoder. This is shown in the following image:\n",
    "![denoising-autoencoder-architecture.png](attachment:denoising-autoencoder-architecture.png)\n",
    "By the way, you'll see this application in detail in the next WOrkshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIOYleyNSauu"
   },
   "source": [
    "Finally, and to conclude on AEs, these networks are very sensitive to the phenomenon of overlearning. To avoid this phenomenon, an improved version of this concept is used today under the name: \"Variational AutoEncoder\". The general concept remains the same: we create an Encoder and a Decoder. However, instead of using scalar functions, we'll define probability functions to encode and decode the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vJbAab-Sauu"
   },
   "source": [
    "### Variational auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvbXWGVBSauu"
   },
   "source": [
    "In this second part, we'll implement a variational autoencoder. Let's start by defining the specific layer we'll use to sample values from the Gaussian distribution defined by the means and standard deviations of the latent space (z_mean, z_logvar).\n",
    "![vae-gaussian.png](attachment:vae-gaussian.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaZVHL2gSauu"
   },
   "outputs": [],
   "source": [
    "# Coding the specific sampling layer as a Keras Layer object\n",
    "class Sampling(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_logvar = inputs\n",
    "\n",
    "        nbatch = K.shape(z_mean)[0]\n",
    "        ndim = K.shape(z_mean)[1]\n",
    "\n",
    "        std = K.exp(z_logvar)\n",
    "        eps = K.random_normal(shape=(nbatch,ndim), mean=0., stddev=0.1)\n",
    "\n",
    "        z = z_mean + eps * std\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cPJFt3tSauu"
   },
   "source": [
    "Now let's write the separate encoder and decoder architectures. Please note: no activation function for averaging and logvar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLgRvHQTSauu"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(784,))\n",
    "hidden1 = layers.Dense(200, activation=\"relu\")(encoder_inputs)\n",
    "\n",
    "z_mean = layers.Dense(10)(hidden1)\n",
    "z_logvar = layers.Dense(10)(hidden1)\n",
    "\n",
    "# Sampling\n",
    "z = Sampling()([z_mean, z_logvar])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, z, name=\"encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSZxDShkSauv"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(encoder, to_file='encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Am--RkBLSauv"
   },
   "outputs": [],
   "source": [
    "# Decoder architecture\n",
    "decoder_inputs = keras.Input(shape=(10,))\n",
    "decoder_hidden = layers.Dense(200, activation=\"relu\")(decoder_inputs)\n",
    "decoder_outputs = layers.Dense(784, activation=\"sigmoid\")(decoder_hidden)\n",
    "\n",
    "decoder = keras.Model(decoder_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1Anwt9ZSauv"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(decoder, to_file='decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBE2vZhhSauv"
   },
   "source": [
    "Now we combine them to build the complete automatic encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVgudtlpSauv"
   },
   "outputs": [],
   "source": [
    "# Combining architectures\n",
    "outputs = decoder(z)\n",
    "vae = keras.Model(encoder_inputs, outputs, name=\"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy0ySLE7Sauv"
   },
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBxgrxzbSauv"
   },
   "source": [
    "We now need to define the learning loss function. As a reconstruction loss, we can always use the binary cross entropy per pixel, summed over the image. In the case of VAE, there is an additional term to the loss: the [Kullback-Leibler divergence](https://fr.wikipedia.org/wiki/Divergence_de_Kullback-Leibler). Write this new term, using Keras backend functions: K.square, K.exp, K.sum...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORnIQsaQSauv"
   },
   "outputs": [],
   "source": [
    "# Loss function definition\n",
    "reconstruction_loss = #PLEASE COMPLETE\n",
    "\n",
    "kl_loss = #PLEASE COMPLETE\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MLZc2rPSauw"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUZXU3rbSauw"
   },
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "history = vae.fit(X_train, X_train,\n",
    "          epochs=30,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erKc949rSauw"
   },
   "outputs": [],
   "source": [
    "# Visualizing the training and validation losses\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uopr7gEBSauw"
   },
   "source": [
    "We seem to have achieved a fairly reasonable training process! Let's have a look at the quality of the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pm5RJO8GSauw"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # Original images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Reconstructed images\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    encoded_img = encoder.predict(np.array([X_test[i]]).reshape(1,784))\n",
    "    decoded_img = decoder.predict(encoded_img).reshape(28, 28)\n",
    "    plt.imshow(decoded_img, cmap=\"gray\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytYeF1HHSaux"
   },
   "source": [
    "We can see that some details are lost, but the overall shapes are correctly reconstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRsiSWnKSaux"
   },
   "source": [
    "As a VAE uses a generative model, we can use the decoder to construct fake images, and we can see that the latent space is continuous! Let's sample a few random images in a given range of latent space values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvDxwrbHSaux"
   },
   "outputs": [],
   "source": [
    "n = 15  # figure with 15x15 generated images\n",
    "figure = np.zeros((28 * n, 28 * n))\n",
    "\n",
    "# We sample n images within [-15, 15] standard deviations, around 0 mean\n",
    "# Vincent:Comment définir les valeurs pour générer les images.\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "\n",
    "        # We sample the latent space over the 2 first neurons, feel free to change that to other pairs!\n",
    "        z_sample = np.array([[xi, yi, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "\n",
    "        digit = x_decoded[0].reshape(28, 28)\n",
    "        figure[i * 28: (i + 1) * 28,\n",
    "               j * 28: (j + 1) * 28] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I23D58vsSaux"
   },
   "outputs": [],
   "source": [
    "x_test_latent = encoder.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
